"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[622],{2821:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"models/quantify-drivers/data","title":"Data","description":"1. Installation","source":"@site/docs/models/quantify-drivers/data.md","sourceDirName":"models/quantify-drivers","slug":"/models/quantify-drivers/data","permalink":"/docs/models/quantify-drivers/data","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/models/quantify-drivers/data.md","tags":[],"version":"current","frontMatter":{"title":"Data"},"sidebar":"tutorialSidebar","previous":{"title":"How to use","permalink":"/docs/models/quantify-drivers/usage"},"next":{"title":"index","permalink":"/docs/models/forest/"}}');var s=i(4848),r=i(8453);const l={title:"Data"},o=void 0,a={},d=[{value:"1. Installation",id:"1-installation",level:2},{value:"1.1 Setup Repository",id:"11-setup-repository",level:3},{value:"1.2 Environment Setup",id:"12-environment-setup",level:3},{value:"2. Training, Evaluation &amp; SHAP computing",id:"2-training-evaluation--shap-computing",level:2},{value:"Configuration",id:"configuration",level:3},{value:"1. Configuration groups",id:"1-configuration-groups",level:4},{value:"2. Key Attributes",id:"2-key-attributes",level:4},{value:"How to use",id:"how-to-use",level:3},{value:"Changing Values via CLI",id:"changing-values-via-cli",level:4},{value:"Creating a New Default",id:"creating-a-new-default",level:4},{value:"Seeds",id:"seeds",level:4},{value:"Path Configuration Guide",id:"path-configuration-guide",level:4},{value:"3. Plots",id:"3-plots",level:2}];function c(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"1-installation",children:"1. Installation"}),"\n",(0,s.jsx)(n.h3,{id:"11-setup-repository",children:"1.1 Setup Repository"}),"\n",(0,s.jsx)(n.p,{children:"Clone the project and navigate to the directory:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"git clone repository_url\ncd repository_name\n"})}),"\n",(0,s.jsx)(n.h3,{id:"12-environment-setup",children:"1.2 Environment Setup"}),"\n",(0,s.jsx)(n.p,{children:"This project uses uv for dependency management."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install uv"})," (if not installed):"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"curl -LsSf https://astral.sh/uv/install.sh | sh\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Note: Restart your shell to ensure uv is in your PATH."})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Create a Local Virtual Environment:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'[ -d ".venv" ] || uv venv\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Init and sync Environment:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"uv init\nuv sync\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Activate venv"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"source .venv/bin/activate\n"})}),"\n",(0,s.jsx)(n.h2,{id:"2-training-evaluation--shap-computing",children:"2. Training, Evaluation & SHAP computing"}),"\n",(0,s.jsx)(n.p,{children:"The workflow for this project follows a pipeline designed to replicate the methodology from the associated paper. The process is divided into four distinct stages:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Data Loading:"})," Retrieves the local-scale (ERA5-Land) and large-scale (ERA5) datasets, applies preprocessing (standardization, lagging), and prepares the PyTorch DataLoaders."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Training:"})," Initializes the Hybrid Model (MLP + ConvNext), handles class imbalance with weighted loss, and trains the model using site-specific hyperparameters."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Evaluation:"})," Loads the trained model weights and computes performance metrics (extreme vs. non-extreme accuracy) on the test set."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"SHAP Computing:"})," Uses shap.GradientExplainer to compute feature importance scores for both the local (NN) and large-scale (CNN) components."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We have two available scripts to do this. The first one (name.py) does all of this process. The second one assumes you have the model weights computed and does only the last steps. How to run, give an example slurm. exaplin this will run on all defaults, which is explained below. emphasis on that specific paths will have to be configured or wont work."}),"\n",(0,s.jsx)(n.p,{children:"This model (you can read the whole paper here) works locally for a certain list of sites: cordoba, lyon, marrakech, belgrado, stockholm and hannover, and does the whole pipeline separately for each of those sites. To configure for which site you want to run the model and other specifics, as the path in which you have saved your data, the code uses hydra configurations. They allow you to whitch between defaults from CLI, and to create your own configuration files. Below is a little guide on how the configuration is structured and how to use. Explain the code shouldnt be changed, only config, in order to obtain the diff results options."}),"\n",(0,s.jsx)(n.p,{children:"There are two primary ways to interact with this pipeline. Both scripts utilize Hydra, meaning you can modify their behavior via the command line without changing the source code."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Full Pipeline (",(0,s.jsx)(n.code,{children:"training_evaluation_SHAP_pipeline.py"}),")"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This script executes the entire lifecycle: it loads data, trains a new model from scratch, evaluates it, and computes SHAP values."}),"\n",(0,s.jsx)(n.p,{children:"Example SLURM Script:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\n#!/bin/bash\n#SBATCH --job-name=train_cordoba\n#SBATCH --output=train_cordoba.out\n#SBATCH --time=02:00:00\n#SBATCH --gres=gpu:1\n\nSCRIPT_PATH="/path/to/training_evaluation_SHAP_pipeline.py"\n\n# Run the pipeline for Cordoba with defaults\nuv run python "$SCRIPT_PATH" site=cordoba\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Note: This will run using the defaults defined in conf/config.yaml. Specific file paths must be configured for your environment (see below)."})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Evaluation & SHAP Only (",(0,s.jsx)(n.code,{children:"evaluation_SHAP_pipeline.py"}),")\nThis script assumes that you have some model weights saved and only does the model evalutaion and SHAP values computing."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["This project uses ",(0,s.jsx)(n.strong,{children:"Hydra"})," for configuration management. This strictly separates the code logic from experimental settings. You should not edit the Python code to change parameters; instead, use the configuration files or CLI overrides."]}),"\n",(0,s.jsxs)(n.p,{children:["The main configuration entry point is ",(0,s.jsx)(n.code,{children:"conf/config.yaml"}),". It is composed of several groups:"]}),"\n",(0,s.jsx)(n.p,{children:"Explanation of all atributes. conf.yaml has this certains parts, with this defaults:"}),"\n",(0,s.jsx)(n.p,{children:"explain hyperparameters, paths, dataset site, seed, percentile. Then dwell more concretely for example in which are the paths that need to be provided. also specify which are the changes that are expected and which would be outside of the reproducibility scope."}),"\n",(0,s.jsx)(n.h4,{id:"1-configuration-groups",children:"1. Configuration groups"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Group"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Description"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Default"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"site"})}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:["The geographical site to model. Supported sites: ",(0,s.jsx)(n.code,{children:"cordoba"}),", ",(0,s.jsx)(n.code,{children:"lyon"}),", ",(0,s.jsx)(n.code,{children:"marrakech"}),", ",(0,s.jsx)(n.code,{children:"belgrado"}),", ",(0,s.jsx)(n.code,{children:"stockholm"}),", ",(0,s.jsx)(n.code,{children:"hannover"}),"."]}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"cordoba"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"dataset"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Defines variables, date ranges, and lag settings for ERA5 and ERA5-Land data."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"default"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"paths"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Crucial: Defines the locations of input NetCDF files and output directories."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"marenostrum"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"hyperparameters"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Specific training settings (learning rate, weight decay, batch size) for the selected site."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"default"})})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"2-key-attributes",children:"2. Key Attributes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"seed"}),": (int) The random seed for reproducibility."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"percentile"}),': (str) The extreme event threshold (e.g., "90p", "95p"). This determines which label file is loaded.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"epoch_config.epochs"}),": (int) Number of training epochs (default: 75)."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"how-to-use",children:"How to use"}),"\n",(0,s.jsx)(n.h4,{id:"changing-values-via-cli",children:"Changing Values via CLI"}),"\n",(0,s.jsx)(n.p,{children:"Hydra allows you to override any config value directly from the command line using dot notation."}),"\n",(0,s.jsxs)(n.p,{children:["Run for a different ",(0,s.jsx)(n.code,{children:"site"})," and ",(0,s.jsx)(n.code,{children:"percentile"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'python training_evaluation_SHAP_pipeline.py site=stockholm percentile="95p"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Run with a specific ",(0,s.jsx)(n.code,{children:"seed"})," and custom ",(0,s.jsx)(n.code,{children:"learning rate"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"python training_evaluation_SHAP_pipeline.py seed=42 hyperparameters.site_hypms.lr=0.0005\n"})}),"\n",(0,s.jsx)(n.h4,{id:"creating-a-new-default",children:"Creating a New Default"}),"\n",(0,s.jsx)(n.p,{children:"If you are moving to a new cluster and need to change the path configuration, or if you are running the code for the first time you can do the followfing:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Create a new file: ",(0,s.jsx)(n.code,{children:"conf/paths/my_cluster.yaml"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Fill in the required paths (base folder, model dir, etc.)."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Run with: ",(0,s.jsx)(n.code,{children:" uv run python training_evaluation_SHAP_pipeline.py paths=my_cluster"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"seeds",children:"Seeds"}),"\n",(0,s.jsxs)(n.p,{children:["To reproduce the paper's ensemble results, you should run the pipeline multiple times with different seeds. The paper uses a standard ensemble of 20 seeds to plot the final results (e.g., generated from a master seed or a fixed list like given below). Hydra allows multirun calls with the -m flag, so you can run the pipeline with ",(0,s.jsx)(n.code,{children:"uv run python training_evaluation_SHAP_pipeline.py -m seed=1234, ...."}),".\nThe list of 20 seeds used in the paper is the following:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"ensemble_seeds = 66316748,2930678936,2546691362,231159514,3904498325,946438445,1095601156,\n791870896,1432871125,755510091,1493800520,3487919346,1938714511,3965736568,1930440936,\n1187877992,3387705611,3520819031,3701866991,3822060012\n"})}),"\n",(0,s.jsx)(n.h4,{id:"path-configuration-guide",children:"Path Configuration Guide"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"paths.yaml"})," file (located in conf/paths/) is the central control room for input data and output locations. The pipeline uses Hydra to load these paths into the configuration.paths object."]}),"\n",(0,s.jsx)(n.p,{children:"You must ensure these paths point to the correct locations on your machine or cluster (e.g., MareNostrum)."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"model_dir"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What it is"}),": The directory where trained model weights (",(0,s.jsx)(n.code,{children:".pth"})," files) are saved after training."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Structure"}),": The script creates subfolders here, e.g., ",(0,s.jsx)(n.code,{children:"model_dir/site/trained_models/"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"results_dir"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What it is"}),": The directory where evaluation metrics, loss histories, and prediction pickles (",(0,s.jsx)(n.code,{children:".pkl"}),") are saved."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Structure"}),": Saves to ",(0,s.jsx)(n.code,{children:"results_dir/site/site_percentile_seed/"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These are only used if use_spei: True is set in the dataset configuration."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"file_spei"}),": Path to the Standardized Precipitation Evapotranspiration Index data."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"file_spi"}),": Path to the Standardized Precipitation Index data."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"3-plots",children:"3. Plots"}),"\n",(0,s.jsx)(n.p,{children:"Explain the notebooks."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function l(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);